{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6-h_7CWusOU"
   },
   "source": [
    "tuning factors : \n",
    "\n",
    "img_size, \n",
    "CNN architechture, \n",
    "validation_set, \n",
    "neuron size, \n",
    "activation functions.\n",
    "\n",
    "things to do\n",
    "\n",
    "2) train with validation set.\n",
    "3) then fine-tune the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pdXCLQG0TDFz"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers,models\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, RMSprop, Adagrad\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yeHkOlpGKLs7",
    "outputId": "62d69b2e-b407-4e33-e96d-8e97db845d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Crack Images:  300\n",
      "Number of No Crack Images:  300\n"
     ]
    }
   ],
   "source": [
    "path='Downloads/archive/train/'\n",
    "trcrimg = os.listdir(path+'Positive/')\n",
    "print(\"Number of Crack Images: \", len(trcrimg))\n",
    "trnocrimg = os.listdir(path+'Negative/')\n",
    "print(\"Number of No Crack Images: \", len(trnocrimg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DRyFlWe4KRVC"
   },
   "outputs": [],
   "source": [
    "labels = ['Negative', 'Positive']\n",
    "def read_images(data_dir):\n",
    "    data = [] \n",
    "    for label in labels: \n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                resized_arr = cv2.resize(img_arr, (img_size, img_size)) \n",
    "                data.append([resized_arr, class_num])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(data)\n",
    "\n",
    "train_set = read_images('Downloads/archive/train/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "a-d7EieqKTq5",
    "outputId": "fdff9947-450f-4d16-988b-6eb7941be21e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Number of Images')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Im = []\n",
    "for i in train_set:\n",
    "    if(i[1] == 0):\n",
    "        Im.append(\"Negative\")\n",
    "    elif(i[1] == 1):\n",
    "        Im.append(\"Positive\")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.set_style('darkgrid')\n",
    "axl = sns.countplot(Im)\n",
    "axl.set_title(\"Number of Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6mq9tZsjKVuR",
    "outputId": "bad4d7f1-044c-428f-cc34-3d55fbd57879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.00392157]\n",
      "   [0.00784314]\n",
      "   [0.10980392]\n",
      "   ...\n",
      "   [0.67058824]\n",
      "   [0.67058824]\n",
      "   [0.74509804]]\n",
      "\n",
      "  [[0.03137255]\n",
      "   [0.0627451 ]\n",
      "   [0.26666667]\n",
      "   ...\n",
      "   [0.61960784]\n",
      "   [0.61176471]\n",
      "   [0.65882353]]\n",
      "\n",
      "  [[0.        ]\n",
      "   [0.16078431]\n",
      "   [0.46666667]\n",
      "   ...\n",
      "   [0.66666667]\n",
      "   [0.60392157]\n",
      "   [0.56470588]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.74509804]\n",
      "   [0.71372549]\n",
      "   [0.69019608]\n",
      "   ...\n",
      "   [0.51372549]\n",
      "   [0.48235294]\n",
      "   [0.53333333]]\n",
      "\n",
      "  [[0.76862745]\n",
      "   [0.70980392]\n",
      "   [0.66666667]\n",
      "   ...\n",
      "   [0.63529412]\n",
      "   [0.48627451]\n",
      "   [0.46666667]]\n",
      "\n",
      "  [[0.74509804]\n",
      "   [0.69019608]\n",
      "   [0.6627451 ]\n",
      "   ...\n",
      "   [0.71372549]\n",
      "   [0.54509804]\n",
      "   [0.50588235]]]\n",
      "\n",
      "\n",
      " [[[0.70196078]\n",
      "   [0.50980392]\n",
      "   [0.52941176]\n",
      "   ...\n",
      "   [0.78431373]\n",
      "   [0.75294118]\n",
      "   [0.72156863]]\n",
      "\n",
      "  [[0.75686275]\n",
      "   [0.70588235]\n",
      "   [0.65882353]\n",
      "   ...\n",
      "   [0.69411765]\n",
      "   [0.69803922]\n",
      "   [0.70196078]]\n",
      "\n",
      "  [[0.81176471]\n",
      "   [0.93333333]\n",
      "   [0.88627451]\n",
      "   ...\n",
      "   [0.61960784]\n",
      "   [0.64313725]\n",
      "   [0.67058824]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6       ]\n",
      "   [0.63921569]\n",
      "   [0.6627451 ]\n",
      "   ...\n",
      "   [0.77254902]\n",
      "   [0.75686275]\n",
      "   [0.7254902 ]]\n",
      "\n",
      "  [[0.6       ]\n",
      "   [0.60392157]\n",
      "   [0.61176471]\n",
      "   ...\n",
      "   [0.74901961]\n",
      "   [0.74509804]\n",
      "   [0.7372549 ]]\n",
      "\n",
      "  [[0.63137255]\n",
      "   [0.58823529]\n",
      "   [0.57647059]\n",
      "   ...\n",
      "   [0.7372549 ]\n",
      "   [0.75294118]\n",
      "   [0.77647059]]]\n",
      "\n",
      "\n",
      " [[[0.71764706]\n",
      "   [0.74509804]\n",
      "   [0.76862745]\n",
      "   ...\n",
      "   [0.97254902]\n",
      "   [0.98039216]\n",
      "   [0.96078431]]\n",
      "\n",
      "  [[0.63921569]\n",
      "   [0.72941176]\n",
      "   [0.81568627]\n",
      "   ...\n",
      "   [0.99607843]\n",
      "   [0.97647059]\n",
      "   [0.94117647]]\n",
      "\n",
      "  [[0.69803922]\n",
      "   [0.77254902]\n",
      "   [0.84313725]\n",
      "   ...\n",
      "   [0.91764706]\n",
      "   [0.95686275]\n",
      "   [0.95294118]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.19607843]\n",
      "   [0.22745098]\n",
      "   [0.54901961]\n",
      "   ...\n",
      "   [0.90588235]\n",
      "   [0.84313725]\n",
      "   [0.68235294]]\n",
      "\n",
      "  [[0.15294118]\n",
      "   [0.20392157]\n",
      "   [0.42352941]\n",
      "   ...\n",
      "   [0.83529412]\n",
      "   [0.85882353]\n",
      "   [0.65098039]]\n",
      "\n",
      "  [[0.28235294]\n",
      "   [0.36470588]\n",
      "   [0.43921569]\n",
      "   ...\n",
      "   [0.87058824]\n",
      "   [0.9254902 ]\n",
      "   [0.8745098 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.71764706]\n",
      "   [0.72156863]\n",
      "   [0.7254902 ]\n",
      "   ...\n",
      "   [0.78431373]\n",
      "   [0.78431373]\n",
      "   [0.78431373]]\n",
      "\n",
      "  [[0.72156863]\n",
      "   [0.72156863]\n",
      "   [0.7254902 ]\n",
      "   ...\n",
      "   [0.79607843]\n",
      "   [0.8       ]\n",
      "   [0.77254902]]\n",
      "\n",
      "  [[0.71764706]\n",
      "   [0.72156863]\n",
      "   [0.72156863]\n",
      "   ...\n",
      "   [0.76078431]\n",
      "   [0.76470588]\n",
      "   [0.71764706]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.70588235]\n",
      "   [0.68627451]\n",
      "   [0.63529412]\n",
      "   ...\n",
      "   [0.74901961]\n",
      "   [0.74117647]\n",
      "   [0.7254902 ]]\n",
      "\n",
      "  [[0.7254902 ]\n",
      "   [0.6627451 ]\n",
      "   [0.63137255]\n",
      "   ...\n",
      "   [0.67843137]\n",
      "   [0.69019608]\n",
      "   [0.69803922]]\n",
      "\n",
      "  [[0.72156863]\n",
      "   [0.65098039]\n",
      "   [0.65882353]\n",
      "   ...\n",
      "   [0.64705882]\n",
      "   [0.67058824]\n",
      "   [0.70196078]]]\n",
      "\n",
      "\n",
      " [[[0.6745098 ]\n",
      "   [0.70196078]\n",
      "   [0.72156863]\n",
      "   ...\n",
      "   [0.2627451 ]\n",
      "   [0.30588235]\n",
      "   [0.31764706]]\n",
      "\n",
      "  [[0.65882353]\n",
      "   [0.67843137]\n",
      "   [0.69803922]\n",
      "   ...\n",
      "   [0.24313725]\n",
      "   [0.27843137]\n",
      "   [0.30588235]]\n",
      "\n",
      "  [[0.65882353]\n",
      "   [0.66666667]\n",
      "   [0.6745098 ]\n",
      "   ...\n",
      "   [0.30980392]\n",
      "   [0.3372549 ]\n",
      "   [0.37254902]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.7254902 ]\n",
      "   [0.7372549 ]\n",
      "   [0.7372549 ]\n",
      "   ...\n",
      "   [0.77254902]\n",
      "   [0.76862745]\n",
      "   [0.76470588]]\n",
      "\n",
      "  [[0.71764706]\n",
      "   [0.74117647]\n",
      "   [0.74901961]\n",
      "   ...\n",
      "   [0.77254902]\n",
      "   [0.76862745]\n",
      "   [0.76470588]]\n",
      "\n",
      "  [[0.70588235]\n",
      "   [0.7372549 ]\n",
      "   [0.75294118]\n",
      "   ...\n",
      "   [0.77254902]\n",
      "   [0.77254902]\n",
      "   [0.76470588]]]\n",
      "\n",
      "\n",
      " [[[0.69411765]\n",
      "   [0.73333333]\n",
      "   [0.76862745]\n",
      "   ...\n",
      "   [0.72156863]\n",
      "   [0.71764706]\n",
      "   [0.70980392]]\n",
      "\n",
      "  [[0.71372549]\n",
      "   [0.7254902 ]\n",
      "   [0.73333333]\n",
      "   ...\n",
      "   [0.72156863]\n",
      "   [0.71764706]\n",
      "   [0.70980392]]\n",
      "\n",
      "  [[0.73333333]\n",
      "   [0.72156863]\n",
      "   [0.70980392]\n",
      "   ...\n",
      "   [0.7254902 ]\n",
      "   [0.72156863]\n",
      "   [0.70980392]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.7254902 ]\n",
      "   [0.72156863]\n",
      "   [0.70980392]\n",
      "   ...\n",
      "   [0.74509804]\n",
      "   [0.71372549]\n",
      "   [0.73333333]]\n",
      "\n",
      "  [[0.70196078]\n",
      "   [0.70196078]\n",
      "   [0.69411765]\n",
      "   ...\n",
      "   [0.72156863]\n",
      "   [0.69803922]\n",
      "   [0.73333333]]\n",
      "\n",
      "  [[0.69803922]\n",
      "   [0.68627451]\n",
      "   [0.66666667]\n",
      "   ...\n",
      "   [0.72156863]\n",
      "   [0.70588235]\n",
      "   [0.75294118]]]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "for feature, label in train_set:\n",
    "    x.append(feature)\n",
    "    y.append(label)\n",
    "x = np.array(x).reshape(-1, img_size, img_size, 1)\n",
    "x = x / 255\n",
    "y = np.array(y)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Negative', 'Positive']\n",
    "def read_images(data_dir):\n",
    "    data = [] \n",
    "    for label in labels: \n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
    "                data.append([resized_arr, class_num])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(data)\n",
    "\n",
    "test_set = read_images('Downloads/archive/test/')\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "for feature, label in test_set:\n",
    "    x_test.append(feature)\n",
    "    y_test.append(label)\n",
    "x_test = np.array(x_test).reshape(-1, img_size, img_size, 1)\n",
    "x_test = x_test / 255\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.56862745]\n",
      "   [0.5372549 ]\n",
      "   [0.54509804]\n",
      "   ...\n",
      "   [0.52941176]\n",
      "   [0.58431373]\n",
      "   [0.6       ]]\n",
      "\n",
      "  [[0.6627451 ]\n",
      "   [0.61960784]\n",
      "   [0.61960784]\n",
      "   ...\n",
      "   [0.45882353]\n",
      "   [0.55686275]\n",
      "   [0.56078431]]\n",
      "\n",
      "  [[0.68627451]\n",
      "   [0.66666667]\n",
      "   [0.68627451]\n",
      "   ...\n",
      "   [0.40392157]\n",
      "   [0.45882353]\n",
      "   [0.47843137]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.50588235]\n",
      "   [0.54117647]\n",
      "   [0.56470588]\n",
      "   ...\n",
      "   [0.57254902]\n",
      "   [0.58431373]\n",
      "   [0.57254902]]\n",
      "\n",
      "  [[0.54117647]\n",
      "   [0.56862745]\n",
      "   [0.58431373]\n",
      "   ...\n",
      "   [0.57647059]\n",
      "   [0.58039216]\n",
      "   [0.56862745]]\n",
      "\n",
      "  [[0.57647059]\n",
      "   [0.58431373]\n",
      "   [0.58431373]\n",
      "   ...\n",
      "   [0.58431373]\n",
      "   [0.59215686]\n",
      "   [0.58431373]]]\n",
      "\n",
      "\n",
      " [[[0.42745098]\n",
      "   [0.49411765]\n",
      "   [0.42745098]\n",
      "   ...\n",
      "   [0.57254902]\n",
      "   [0.60392157]\n",
      "   [0.57254902]]\n",
      "\n",
      "  [[0.57254902]\n",
      "   [0.43529412]\n",
      "   [0.52156863]\n",
      "   ...\n",
      "   [0.51764706]\n",
      "   [0.60784314]\n",
      "   [0.52941176]]\n",
      "\n",
      "  [[0.58431373]\n",
      "   [0.54901961]\n",
      "   [0.61568627]\n",
      "   ...\n",
      "   [0.4627451 ]\n",
      "   [0.63137255]\n",
      "   [0.55686275]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.39607843]\n",
      "   [0.46666667]\n",
      "   [0.58431373]\n",
      "   ...\n",
      "   [0.46666667]\n",
      "   [0.44313725]\n",
      "   [0.50980392]]\n",
      "\n",
      "  [[0.42352941]\n",
      "   [0.56078431]\n",
      "   [0.61960784]\n",
      "   ...\n",
      "   [0.40392157]\n",
      "   [0.41568627]\n",
      "   [0.42352941]]\n",
      "\n",
      "  [[0.50196078]\n",
      "   [0.60784314]\n",
      "   [0.49411765]\n",
      "   ...\n",
      "   [0.29411765]\n",
      "   [0.44313725]\n",
      "   [0.50980392]]]\n",
      "\n",
      "\n",
      " [[[0.75686275]\n",
      "   [0.64705882]\n",
      "   [0.55294118]\n",
      "   ...\n",
      "   [0.56078431]\n",
      "   [0.59607843]\n",
      "   [0.63921569]]\n",
      "\n",
      "  [[0.70980392]\n",
      "   [0.69803922]\n",
      "   [0.61568627]\n",
      "   ...\n",
      "   [0.50196078]\n",
      "   [0.42745098]\n",
      "   [0.46666667]]\n",
      "\n",
      "  [[0.72156863]\n",
      "   [0.69411765]\n",
      "   [0.69411765]\n",
      "   ...\n",
      "   [0.48235294]\n",
      "   [0.36862745]\n",
      "   [0.44313725]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.67058824]\n",
      "   [0.69019608]\n",
      "   [0.70196078]\n",
      "   ...\n",
      "   [0.74901961]\n",
      "   [0.7254902 ]\n",
      "   [0.71764706]]\n",
      "\n",
      "  [[0.67058824]\n",
      "   [0.68235294]\n",
      "   [0.68235294]\n",
      "   ...\n",
      "   [0.69411765]\n",
      "   [0.60784314]\n",
      "   [0.65882353]]\n",
      "\n",
      "  [[0.72156863]\n",
      "   [0.71764706]\n",
      "   [0.68627451]\n",
      "   ...\n",
      "   [0.48235294]\n",
      "   [0.37647059]\n",
      "   [0.48627451]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.77647059]\n",
      "   [0.68627451]\n",
      "   [0.50980392]\n",
      "   ...\n",
      "   [0.52156863]\n",
      "   [0.58823529]\n",
      "   [0.60392157]]\n",
      "\n",
      "  [[0.74117647]\n",
      "   [0.48235294]\n",
      "   [0.44313725]\n",
      "   ...\n",
      "   [0.43921569]\n",
      "   [0.60392157]\n",
      "   [0.68235294]]\n",
      "\n",
      "  [[0.36078431]\n",
      "   [0.34901961]\n",
      "   [0.60784314]\n",
      "   ...\n",
      "   [0.45882353]\n",
      "   [0.52941176]\n",
      "   [0.5372549 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.71372549]\n",
      "   [0.71764706]\n",
      "   [0.71372549]\n",
      "   ...\n",
      "   [0.70196078]\n",
      "   [0.59215686]\n",
      "   [0.44313725]]\n",
      "\n",
      "  [[0.74117647]\n",
      "   [0.63921569]\n",
      "   [0.62352941]\n",
      "   ...\n",
      "   [0.71764706]\n",
      "   [0.64313725]\n",
      "   [0.58823529]]\n",
      "\n",
      "  [[0.69019608]\n",
      "   [0.65882353]\n",
      "   [0.72156863]\n",
      "   ...\n",
      "   [0.70588235]\n",
      "   [0.63921569]\n",
      "   [0.70196078]]]\n",
      "\n",
      "\n",
      " [[[0.77647059]\n",
      "   [0.66666667]\n",
      "   [0.62745098]\n",
      "   ...\n",
      "   [0.40392157]\n",
      "   [0.44313725]\n",
      "   [0.46666667]]\n",
      "\n",
      "  [[0.77647059]\n",
      "   [0.67843137]\n",
      "   [0.63137255]\n",
      "   ...\n",
      "   [0.4       ]\n",
      "   [0.41960784]\n",
      "   [0.41176471]]\n",
      "\n",
      "  [[0.73333333]\n",
      "   [0.6627451 ]\n",
      "   [0.63137255]\n",
      "   ...\n",
      "   [0.38823529]\n",
      "   [0.45882353]\n",
      "   [0.4745098 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.69019608]\n",
      "   [0.69411765]\n",
      "   [0.69803922]\n",
      "   ...\n",
      "   [0.15686275]\n",
      "   [0.22745098]\n",
      "   [0.35294118]]\n",
      "\n",
      "  [[0.61176471]\n",
      "   [0.64313725]\n",
      "   [0.69019608]\n",
      "   ...\n",
      "   [0.1372549 ]\n",
      "   [0.2       ]\n",
      "   [0.36078431]]\n",
      "\n",
      "  [[0.57647059]\n",
      "   [0.62745098]\n",
      "   [0.70196078]\n",
      "   ...\n",
      "   [0.31764706]\n",
      "   [0.27058824]\n",
      "   [0.33333333]]]\n",
      "\n",
      "\n",
      " [[[0.1372549 ]\n",
      "   [0.25490196]\n",
      "   [0.44705882]\n",
      "   ...\n",
      "   [0.72941176]\n",
      "   [0.76078431]\n",
      "   [0.83529412]]\n",
      "\n",
      "  [[0.38823529]\n",
      "   [0.31372549]\n",
      "   [0.43921569]\n",
      "   ...\n",
      "   [0.72941176]\n",
      "   [0.70980392]\n",
      "   [0.71764706]]\n",
      "\n",
      "  [[0.59215686]\n",
      "   [0.4745098 ]\n",
      "   [0.50980392]\n",
      "   ...\n",
      "   [0.74509804]\n",
      "   [0.63921569]\n",
      "   [0.55294118]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.76470588]\n",
      "   [0.64313725]\n",
      "   [0.65490196]\n",
      "   ...\n",
      "   [0.72941176]\n",
      "   [0.67843137]\n",
      "   [0.71372549]]\n",
      "\n",
      "  [[0.76078431]\n",
      "   [0.65098039]\n",
      "   [0.65490196]\n",
      "   ...\n",
      "   [0.61176471]\n",
      "   [0.60392157]\n",
      "   [0.71372549]]\n",
      "\n",
      "  [[0.8       ]\n",
      "   [0.69803922]\n",
      "   [0.6627451 ]\n",
      "   ...\n",
      "   [0.56078431]\n",
      "   [0.61960784]\n",
      "   [0.80392157]]]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "labels = ['Negative', 'Positive']\n",
    "def read_images(data_dir):\n",
    "    data = [] \n",
    "    for label in labels: \n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                resized_arr = cv2.resize(img_arr, (img_size, img_size)) \n",
    "                data.append([resized_arr, class_num])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(data)\n",
    "\n",
    "valid_set = read_images('Downloads/archive/valid/')\n",
    "x_valid = []\n",
    "y_valid = []\n",
    "for feature, label in valid_set:\n",
    "    x_valid.append(feature)\n",
    "    y_valid.append(label)\n",
    "x_valid = np.array(x_valid).reshape(-1, img_size, img_size, 1)\n",
    "x_valid = x_valid / 255\n",
    "y_valid = np.array(y_valid)\n",
    "print(x_valid)\n",
    "print(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fr2JjKElKV1i",
    "outputId": "02bf9841-213d-4cd8-f87d-e7b8fd781d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_15 (Conv2D)          (None, 222, 222, 64)      640       \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 111, 111, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 109, 109, 128)     73856     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 54, 54, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 373248)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 746498    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 820,994\n",
      "Trainable params: 820,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 52s 3s/step - loss: 1.6163 - accuracy: 0.4667 - val_loss: 0.5614 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 50s 3s/step - loss: 0.5212 - accuracy: 0.6633 - val_loss: 0.4306 - val_accuracy: 0.8400\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 52s 3s/step - loss: 0.4984 - accuracy: 0.8133 - val_loss: 0.4631 - val_accuracy: 0.9250\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 50s 3s/step - loss: 0.4631 - accuracy: 0.8433 - val_loss: 0.4058 - val_accuracy: 0.9300\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 50s 3s/step - loss: 0.4268 - accuracy: 0.8950 - val_loss: 0.3939 - val_accuracy: 0.8950\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 52s 3s/step - loss: 0.3932 - accuracy: 0.9100 - val_loss: 0.3791 - val_accuracy: 0.9250\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 53s 3s/step - loss: 0.3485 - accuracy: 0.9517 - val_loss: 0.3711 - val_accuracy: 0.9200\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 53s 3s/step - loss: 0.2184 - accuracy: 0.9633 - val_loss: 0.2360 - val_accuracy: 0.9400\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 42s 2s/step - loss: 0.2199 - accuracy: 0.9333 - val_loss: 0.6360 - val_accuracy: 0.8450\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 55s 3s/step - loss: 0.2628 - accuracy: 0.9383 - val_loss: 0.2345 - val_accuracy: 0.9300\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 53s 3s/step - loss: 0.0935 - accuracy: 0.9717 - val_loss: 0.2332 - val_accuracy: 0.9350\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 60s 3s/step - loss: 0.0634 - accuracy: 0.9833 - val_loss: 0.2548 - val_accuracy: 0.9450\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 55s 3s/step - loss: 0.0286 - accuracy: 0.9950 - val_loss: 0.4507 - val_accuracy: 0.9150\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 57s 3s/step - loss: 0.0710 - accuracy: 0.9917 - val_loss: 0.4300 - val_accuracy: 0.9250\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 54s 3s/step - loss: 0.0255 - accuracy: 0.9950 - val_loss: 0.4648 - val_accuracy: 0.9250\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 47s 2s/step - loss: 0.0114 - accuracy: 0.9983 - val_loss: 0.5314 - val_accuracy: 0.9100\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.5778 - val_accuracy: 0.9100\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 42s 2s/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.6150 - val_accuracy: 0.9150\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 43s 2s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6527 - val_accuracy: 0.9200\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 42s 2s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6872 - val_accuracy: 0.9100\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 42s 2s/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.6974 - val_accuracy: 0.9000\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 39s 2s/step - loss: 7.5920e-04 - accuracy: 1.0000 - val_loss: 0.7358 - val_accuracy: 0.8950\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 36s 2s/step - loss: 9.0086e-04 - accuracy: 1.0000 - val_loss: 0.7639 - val_accuracy: 0.9000\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 44s 2s/step - loss: 7.7226e-04 - accuracy: 1.0000 - val_loss: 0.8055 - val_accuracy: 0.9000\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 6.0997e-04 - accuracy: 1.0000 - val_loss: 0.8007 - val_accuracy: 0.9000\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 4.7685e-04 - accuracy: 1.0000 - val_loss: 0.8363 - val_accuracy: 0.9000\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 3.8109e-04 - accuracy: 1.0000 - val_loss: 0.8579 - val_accuracy: 0.9000\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 34s 2s/step - loss: 3.2632e-04 - accuracy: 1.0000 - val_loss: 0.8705 - val_accuracy: 0.8900\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 34s 2s/step - loss: 2.8345e-04 - accuracy: 1.0000 - val_loss: 0.8971 - val_accuracy: 0.9000\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 2.5001e-04 - accuracy: 1.0000 - val_loss: 0.9071 - val_accuracy: 0.8900\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 2.1980e-04 - accuracy: 1.0000 - val_loss: 0.9249 - val_accuracy: 0.8900\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 34s 2s/step - loss: 1.9560e-04 - accuracy: 1.0000 - val_loss: 0.9380 - val_accuracy: 0.8900\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 1.7631e-04 - accuracy: 1.0000 - val_loss: 0.9481 - val_accuracy: 0.8900\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 1.6020e-04 - accuracy: 1.0000 - val_loss: 0.9644 - val_accuracy: 0.8900\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 42s 2s/step - loss: 1.5104e-04 - accuracy: 1.0000 - val_loss: 0.9743 - val_accuracy: 0.8900\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 50s 3s/step - loss: 1.3418e-04 - accuracy: 1.0000 - val_loss: 0.9874 - val_accuracy: 0.8900\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 58s 3s/step - loss: 1.2040e-04 - accuracy: 1.0000 - val_loss: 0.9974 - val_accuracy: 0.8900\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 53s 3s/step - loss: 1.1190e-04 - accuracy: 1.0000 - val_loss: 1.0091 - val_accuracy: 0.8900\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 43s 2s/step - loss: 1.0266e-04 - accuracy: 1.0000 - val_loss: 1.0189 - val_accuracy: 0.8900\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 35s 2s/step - loss: 9.3776e-05 - accuracy: 1.0000 - val_loss: 1.0262 - val_accuracy: 0.8900\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 37s 2s/step - loss: 8.7723e-05 - accuracy: 1.0000 - val_loss: 1.0364 - val_accuracy: 0.8900\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 8.1842e-05 - accuracy: 1.0000 - val_loss: 1.0458 - val_accuracy: 0.8900\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 7.6782e-05 - accuracy: 1.0000 - val_loss: 1.0599 - val_accuracy: 0.8900\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 7.2501e-05 - accuracy: 1.0000 - val_loss: 1.0627 - val_accuracy: 0.8900\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 34s 2s/step - loss: 6.8118e-05 - accuracy: 1.0000 - val_loss: 1.0711 - val_accuracy: 0.8900\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 6.4440e-05 - accuracy: 1.0000 - val_loss: 1.0817 - val_accuracy: 0.8900\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 6.1377e-05 - accuracy: 1.0000 - val_loss: 1.0902 - val_accuracy: 0.8900\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 5.7320e-05 - accuracy: 1.0000 - val_loss: 1.0964 - val_accuracy: 0.8900\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 5.4336e-05 - accuracy: 1.0000 - val_loss: 1.1034 - val_accuracy: 0.8900\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 5.1678e-05 - accuracy: 1.0000 - val_loss: 1.1104 - val_accuracy: 0.8900\n",
      "7/7 [==============================] - 3s 390ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.57      1.00      0.73       100\n",
      "    Positive       1.00      0.26      0.41       100\n",
      "\n",
      "    accuracy                           0.63       200\n",
      "   macro avg       0.79      0.63      0.57       200\n",
      "weighted avg       0.79      0.63      0.57       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape= x.shape[1:]))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(2))\n",
    "model.summary()\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt,loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "history = model.fit(x, y, epochs = 50,validation_data = (x_valid, y_valid), verbose=1)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "predictions = model.predict(x_test)\n",
    "predictions=np.argmax(predictions,axis=1)\n",
    "predictions = predictions.reshape(1,-1)[0]\n",
    "print(classification_report(y_test, predictions, target_names = ['Negative','Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
